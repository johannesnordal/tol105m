#!/usr/bin/env bash

#SBATCH --partition=gpu
#SBATCH --cluster=wice
#SBATCH -A lp_wice_pilot
#SBATCH --nodes=2
#SBATCH -t 24:00:00
#SBATCH --gpus-per-node=4
#SBATCH --ntasks-per-node=72

#SBATCH --job-name=wice_test
#SBATCH --output=logs/%x.%j.out
#SBATCH --error=logs/%x.%j.err

ip a
nvidia-smi
nvidia-smi -a

task="batch4_sp"
data_fn="train_batch4.hdf5"

export MASTER_ADDR=$(scontrol show hostname ${SLURM_NODELIST} | head -n 1)
export MASTER_PORT=29500
export WORLD_SIZE=1 #$((SLURM_NNODES * SLURM_GPUS_PER_NODE))
export TORCH_HOME=/raise/python/nn/$task/weights

echo 'copying data locally ...'
srun cp $DATA/$data_fn $TMPDIR/$data_fn
echo 'local copy done'

ls /nvme/scratch/

srun singularity exec --nv \
                  --pwd /raise/python/nn/$task \
                  --bind /scratch:/user,$PROJECT/raise:/raise,$TMPDIR/$data_fn:/data/dataset.hdf5,$PROJECT/raise/clearml_cache:/clearml_cache \
                  $PROJECT/raise_nn.sif python -u train.py ./configs/MViT.yaml