#!/bin/bash

# slurm job configuration
#SBATCH --nodes=8
#SBATCH --ntasks=8
#SBATCH --output=job.out
#SBATCH --error=job.err
#SBATCH --time=00:30:00
#SBATCH --job-name=test
#SBATCH --gres=gpu:1 
#SBATCH --partition=dp-esb-ib

# load modules
module --force purge
module use $OTHERSTAGES 
ml Stages/2020  GCC ParaStationMPI/5.4.7-1-mt Python

# source env
source /p/project/prcoe12/RAISE/envAI_deepv/bin/activate

# cuda flags
export CUDA_VISIBLE_DEVICES="0,1,2,3"
# export CUDA_LAUNCH_BLOCKING=1 # blocks launch, no need at this moment

# file to execute
COMMAND="Autoencoder_Turbulence_horv.py --batch_size 100000 --epochs 3 --learning_rate 0.001"

# debug features
debug=false

# sleep a sec
sleep 1

# Echo job configuration
if [ "$debug" = true ] ; then
   echo "DEBUG: SLURM_JOB_NODELIST=$SLURM_JOB_NODELIST"
   echo "DEBUG: SLURM_NNODES=$SLURM_NNODES"
   echo "DEBUG: SLURM_NTASKS=$SLURM_NTASKS"
   echo "DEBUG: SLURM_TASKS_PER_NODE=$SLURM_TASKS_PER_NODE"
   echo "DEBUG: SLURM_SUBMIT_HOST=$SLURM_SUBMIT_HOST"
   echo "DEBUG: SLURM_NODEID=$SLURM_NODEID"
   echo "DEBUG: SLURM_LOCALID=$SLURM_LOCALID" 
   echo "DEBUG: SLURM_PROCID=$SLURM_PROCID"
fi

# set comm
PSP_CUDA=1
PSP_UCP=1 
export NCCL_SOCKET_IFNAME=ib
export NCCL_IB_HCA=ipogif0 
export NCCL_IB_CUDA_SUPPORT=1
export CUDA_VISIBLE_DEVICES="0,1,2,3"
if [ "$debug" = true ] ; then
  export NCCL_DEBUG=VERSION
  export NCCL_DEBUG=INFO
  export NCCL_DEBUG_SUBSYS=ALL
fi

# launch
srun --cpu-bind=none python -u  $COMMAND

# eof
